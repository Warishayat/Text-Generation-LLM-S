{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U accelerate\n!pip install -q -U datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin('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')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map={\"\":0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.gradient_checkpointing_enable()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndata = load_dataset(\"Amod/mental_health_counseling_conversations\")\ndata.column_names\ntrain_test_split = data['train'].train_test_split(test_size=0.3)\ntrain_dataset = train_test_split['train']\nvalidation_dataset = train_test_split['test']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_dataset.map(\n    lambda samples: tokenizer(\n        text=samples[\"Context\"],\n        text_pair=samples[\"Response\"],  # Use text_pair for the second input\n        padding=True,\n        truncation=True,\n        max_length=512\n    ),\n    batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef check_model_parameters(model):\n    \"\"\"\n    Check trainable, non-trainable, and frozen parameters of the model.\n\n    Args:\n        model (torch.nn.Module): The model to check.\n\n    Returns:\n        dict: A dictionary with counts of trainable, non-trainable, and frozen parameters.\n    \"\"\"\n    total_params = 0\n    trainable_params = 0\n    non_trainable_params = 0\n\n    for name, param in model.named_parameters():\n        param_count = param.numel()  # Get the number of elements in the parameter\n        total_params += param_count\n\n        if param.requires_grad:\n            trainable_params += param_count\n        else:\n            non_trainable_params += param_count\n\n    frozen_params = total_params - trainable_params\n\n    return {\n        \"Total Parameters\": total_params,\n        \"Trainable Parameters\": trainable_params,\n        \"Non-Trainable Parameters\": non_trainable_params,\n        \"Frozen Parameters\": frozen_params,\n    }\n\n# Example usage:\n# model = LlamaForCausalLM.from_pretrained(\"path/to/your/model\")\n# params_info = check_model_parameters(model)\n# print(params_info)\n\n\ncheck_model_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n\n# Prepare the model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n\n# Create the Lora configuration with the correct target modules\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=32,\n   target_modules=[\"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Get the PEFT model\nmodel = get_peft_model(model, config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer as TransformersTrainer, TrainingArguments,Trainer\n\n# Prepare training arguments\ntraining_args = TrainingArguments(\n    output_dir=r\"C:\\Users\\HP\\Desktop\\Text-Generation\\Trained-Model-saved\",\n    per_device_train_batch_size=2,\n    num_train_epochs=1,\n    warmup_steps=10,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    bf16=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=data,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_directory = r'C:\\Users\\HP\\Desktop\\Text-Generation\\Model-performance'\nos.makedirs(save_directory, exist_ok=True)  # Create directory if it doesn't exist\n\n# Try saving the model and tokenizer\ntry:\n    model.save_pretrained(save_directory)\n    tokenizer.save_pretrained(save_directory)\n    print(f\"Model and tokenizer saved to {save_directory}\")\n\n    # Verify saved files\n    files = os.listdir(save_directory)\n    print(\"Files in save directory:\", files)\nexcept Exception as e:\n    print(f\"Error saving model: {e}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# Replace 'model-performance' with your model directory name\nshutil.make_archive('model-performance', 'zip', '/kaggle/working/model-performance')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Saving model and tokenizer to:\", save_directory)\nimport os\nfor i in os.listdir(save_directory):\n    print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_directory = \"/kaggle/working/model-performance\"\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\nfrom torch.utils.tensorboard import SummaryWriter\nimport os\n\n# Ensure validation_dataset is defined and contains expected fields\nif 'Context' not in validation_dataset.column_names or 'Response' not in validation_dataset.column_names:\n    raise ValueError(\"Validation dataset must contain 'Context' and 'Response' fields.\")\n\neval_data = validation_dataset.map(\n    lambda samples: tokenizer(\n        text=samples[\"Context\"],\n        text_pair=samples[\"Response\"],\n        padding=True,\n        truncation=True,\n        max_length=512\n    ),\n    batched=True\n)\n\n# Initialize the Trainer\neval_trainer = Trainer(\n    model=model,\n    args=training_args,\n    eval_dataset=eval_data,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\n\n# Evaluate the model\nresult = None  # Initialize result\ntry:\n    result = eval_trainer.evaluate()\n    print(result)\nexcept Exception as e:\n    print(\"Error during evaluation:\", e)\n\n# Check if result is defined before logging\nif result is not None:\n    # Initialize TensorBoard writer\n    log_dir = r'C:\\Users\\HP\\Desktop\\Text-Generation\\Model-performance'\n    os.makedirs(log_dir, exist_ok=True)\n    writer = SummaryWriter(log_dir)\n\n    # Log evaluation metrics\n    for key, value in result.items():\n        writer.add_scalar(f'Evaluation/{key}', value)\n\n    # Close the writer\n    writer.close()\n\n    print(\"Saved all things successfully.\")\nelse:\n    print(\"Evaluation did not complete successfully; no results to save.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}